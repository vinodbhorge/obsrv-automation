# vim: set fdm=indent:
nameOverride: ""
fullnameOverride: ""

replicaCount: 1

namespace: "flink"
commonLabels:
  system.processing: "true"
  release: monitoring
  app: flink


# repository: sunbirded.azurecr.io/data-pipeline
#   tag: "release-5.2.0_RC1_2c615f8_12"
# docker pull sunbirded.azurecr.io/sunbird-datapipeline:release-4.9.0_RC4_1
registry: sanketikahub
repository: merged-pipeline
tag: 1.0.0-GA
imagePullPolicy: IfNotPresent
imagePullSecrets: []

## Databases
global:
  redis:
    host: valkey-denorm-headless.redis.svc.cluster.local
    port: 6379
  cassandra:
    host: localhost
    port: 9042
  kafka:
    host: "kafka-headless.kafka.svc.cluster.local"
    port: 9092
  zookeeper:
    host: "kafka-zookeeper-headless.kafka.svc.cluster.local"
    port: 2181
  image:
    registry: ""

podAnnotations: {}

podSecurityContext:
  # runAsNonRoot: true
  #runAsUser: 0
  #fsGroup: 0

securityContext:
  {}
  # readOnlyRootFilesystem: false
  # capabilities:
  #   drop:
  #   - ALL

service:
  type: ClusterIP
  ports:
    - name: http
      port: 8081
      targetPort: 8081

ingress:
  enabled: false
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - /

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

nodeSelector: {}
tolerations: []
affinity: {}

configmap:
  enabled: false
  mountPath: /config

serviceAccount:
  create: true
  name: ""

# Example values.yaml structure
initContainers:
  {}
  # - name: init-myservice
  #   image: busybox:1.28
  #   command: ['sh', '-c', "until nslookup kubernetes.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]

sidecars:
  {}
  # - name: log-reader # Sidecar container
  #   image: busybox # Use another busybox image
  #   command: ["/bin/sh"] # Override the default command
  #   args: ["-c", "tail -f /var/log/app.log"] # Run a shell script that tails the log file

opa:
  enabled: false

jobmanager:
  rpc_port: 6123
  blob_port: 6124
  query_port: 6125
  ui_port: 8081
  prom_port: 9250
  heap_memory: 1024

rest_port: 80
resttcp_port: 8081

taskmanager:
  prom_port: 9251
  rpc_port: 6122
  heap_memory: 1024
  replicas: 1
  cpu_requests: 0.3

checkpoint_store_type: null

# AWS S3 Details
s3_auth_type: "serviceAccount"
s3_access_key: ""
s3_secret_key: ""
s3_endpoint: ""
s3_path_style_access: ""

# Azure Container Details
azure_account: "azure-test"
azure_secret: "azure-secret"

# Azure Container Details
cloud_storage_flink_bucketname: flink-state-backend
cloud_storage_content_bucketname: sunbird-content-dev
cert_container_name: dev-e-credentials
# cloud_storage_endpoint: https://{{ .Values.global.azure_storage_account_name }}.blob.core.windows.net

flink_dp_storage_container: ""
checkpoint_interval: 60000
checkpoint_pause_between_seconds: 5000
checkpoint_compression_enabled: true
restart_attempts: 3
restart_delay: 30000 # in milli-seconds
producer_max_request_size: 1572864
producer_batch_size: 98304
producer_linger_ms: 10
producer_compression: snappy
extractor_event_max_size: 1048576 # Max is only 1MB
extractor_max_request_size: 5242880
extractor_consumer_parallelism: 1
extractor_operators_parallelism: 1
telemetry_extractor_key_expiry_seconds: 3600
ingest_router_consumer_parallelism: 1
ingest_router_operators_parallelism: 1
raw_router_consumer_parallelism: 1
raw_router_downstream_parallelism: 1

content_port: 6379
device_port: 6380
user_port: 6381
dialcode_port: 6382

pipeline_preprocessor_consumer_parallelism: 1
pipeline_preprocessor_operators_parallelism: 1
portal_id: ".sunbird.portal"
desktop_id: ".sunbird.desktop"
pipeline_preprocessor_key_expiry_seconds: 3600

denorm_secondary_window_count: 30
denorm_secondary_window_shards: 1400
denorm_primary_window_count: 30
denorm_primary_window_shards: 1400

denorm_summary_window_count: 5
denorm_summary_window_shards: 1400

denorm_secondary_consumer_parallelism: 1
telemetry_denorm_secondary_operators_parallelism: 1
denorm_primary_consumer_parallelism: 1
telemetry_denorm_primary_operators_parallelism: 1

### summary-denormalization related vars
summary_denorm_consumer_parallelism: 1
summary_denorm_operators_parallelism: 1
summary_denorm_duplication_key_expiry_seconds: 3600
summary_denorm_key_expiry_seconds: 3600

### De-normalization related vars
denorm_consumer_parallelism: 1
telemetry_denorm_operators_parallelism: 1
de_normalization_duplicationstore_key_expiry_seconds: 3600
de_normalization_key_expiry_seconds: 3600
denorm_window_count: 30
denorm_window_shards: 1400

### Druid-validator related vars
druid_validator_consumer_parallelism: 1
druid_validator_operators_parallelism: 1
druid_validator_key_expiry_seconds: 3600
druid_validation_enabled: true
druid_deduplication_enabled: true

### error-denormalization related vars
error_denorm_consumer_parallelism: 1
error_denorm_operators_parallelism: 1

### Device-profile-updater related vars
deviceprofile_parallelism: 1
device_profile_updater_key_expiry_seconds: 3600
device_profile_table: "_device_profile"

### content-cache-updater
# dialcode_api_url: {{ .Values.global.proto }}://{{ .Values.global.domain_name }}/{{ .Values.dialcode_endpoint }}
dialcode_api_auth_key: ""

#(user read api details)
user_read_api_endpoint: "/private/user/v1/read/"
user_read_api_url: "learner-service:9000"

### User-cache-updater related vars
usercache_updater_parallelism: 1
user_cache_updater_key_expiry_seconds: 3600
middleware_cassandra_keyspace: sunbird
middleware_cassandra_user_table: user
middleware_cassandra_location_table: location

postgres:
  max_connections: 2
  sslmode: false
  db_name: analytics
  db_port: 5432

checkpointing:
  enabled: false
  statebackend: null

log4j_console_properties: |
  # This affects logging for both user code and Flink
  rootLogger.level = INFO
  rootLogger.appenderRef.console.ref = ConsoleAppender
  #rootLogger.appenderRef.rolling.ref = RollingFileAppender

  # Uncomment this if you want to _only_ change Flink's logging
  logger.flink.name = org.apache.flink
  logger.flink.level = INFO

  # The following lines keep the log level of common libraries/connectors on
  # log level INFO. The root logger does not override this. You have to manually
  # change the log levels here.
  logger.akka.name = akka
  logger.akka.level = ERROR
  logger.kafka.name= org.apache.kafka
  logger.kafka.level = ERROR
  logger.hadoop.name = org.apache.hadoop
  logger.hadoop.level = ERROR
  logger.zookeeper.name = org.apache.zookeeper
  logger.zookeeper.level = ERROR

  # Log all infos to the console
  appender.console.name = ConsoleAppender
  appender.console.type = CONSOLE
  appender.console.layout.type = PatternLayout
  appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

  # Log all infos in the given rolling file
  appender.rolling.name = RollingFileAppender
  appender.rolling.type = RollingFile
  appender.rolling.append = false
  appender.rolling.fileName = ${sys:log.file}
  appender.rolling.filePattern = ${sys:log.file}.%i
  appender.rolling.layout.type = PatternLayout
  appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
  appender.rolling.policies.type = Policies
  appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
  appender.rolling.policies.size.size=10MB
  appender.rolling.strategy.type = DefaultRolloverStrategy
  appender.rolling.strategy.max = 5

  # Suppress the irrelevant (wrong) warnings from the Netty channel handler
  logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
  logger.netty.level = OFF

baseconfig: |
  kafka {
    broker-servers = "{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
    producer.broker-servers = "{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
    consumer.broker-servers = "{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
    zookeeper = "{{ .Values.global.zookeeper.host }}:{{ .Values.global.zookeeper.port }}"
    producer {
      max-request-size = 1572864
      batch.size = 98304
      linger.ms = 10
      compression = "snappy"
    }
    output.system.event.topic = "system.events"

    output.failed.topic = "failed"
  }
  job {
    enable.distributed.checkpointing = {{ .Values.checkpointing.enabled }}
    statebackend {
      base.url = "{{ .Values.checkpointing.statebackend }}"
    }
  }

  task {
    parallelism = 1
    consumer.parallelism = 1
    checkpointing.interval = 10000
    checkpointing.pause.between.seconds = 10000
    restart-strategy.attempts = 3
    restart-strategy.delay = 30000 # in milli-seconds
  }

  redis.connection.timeout = 100
  redis {
    host = "{{ .Values.global.valkey_dedup.host }}"
    port = {{ .Values.global.valkey_dedup.port }}
  }

  redis-meta {
    host = "{{ .Values.global.valkey_denorm.host }}"
    port = {{ .Values.global.valkey_denorm.port }}
  }

  postgres {
    host = "{{ .Values.global.postgresql.host }}"
    port = "{{ .Values.global.postgresql.port }}"
    maxConnections = "{{ .Values.postgres.max_connections }}"
    sslMode = "{{ .Values.postgres.sslmode }}"
    user = "{{ .Values.global.postgresql.obsrv.user }}"
    password = "{{ .Values.global.postgresql.obsrv.password }}"
    database = "{{ .Values.global.postgresql.obsrv.name }}"
  }

  lms-cassandra {
    host = "{{ .Values.global.cassandra.host }}"
    port = "{{ .Values.global.cassandra.port }}"
  }

  otel.collector.endpoint="http://opentelemetry-collector.otel.svc.cluster.local:4317"
  otel.enable = false

serviceMonitor: &serviceMonitor
  jobmanager:
    enabled: true
    interval: 30s
    scrapeTimeout: 10s
    labels: {} # additional labels e.g. release: prometheus
    honorLabels: true
    jobLabel: "app.kubernetes.io/name"
    port: prom
  taskmanager:
    enabled: true
    interval: 30s
    scrapeTimeout: 10s
    labels: {} # additional labels e.g. release: prometheus
    honorLabels: true
    jobLabel: "app.kubernetes.io/name"
    port: prom

flink_jobs:
  unified-pipeline:
    enabled: false

    registry: sanketikahub
    repository: unified-pipeline
    tag: 1.0.0-GA
    imagePullSecrets: []

    job_classname: org.sunbird.obsrv.pipeline.task.UnifiedPipelineStreamTask

    serviceMonitor: *serviceMonitor

    config: |+
      include file("/data/flink/conf/baseconfig.conf")
      kafka {
        input.topic = "ingest"
        output.raw.topic = "raw"
        output.extractor.duplicate.topic = "failed"
        output.batch.failed.topic = "failed"
        event.max.size = "1048576" # Max is only 1MB
        output.invalid.topic = "failed"
        output.unique.topic = "unique"
        output.duplicate.topic = "failed"
        output.denorm.topic = "denorm"
        output.denorm.failed.topic = "failed"
        output.transform.topic = "transform"
        output.transform.failed.topic = "transform.failed"
        stats.topic = "stats"
        groupId = "unified-pipeline-group"
        producer {
          max-request-size = 5242880
        }
      }

      task {
        window.time.in.seconds = 5
        window.count = 30
        window.shards = 1400
        consumer.parallelism = 2
        downstream.operators.parallelism = 2
      }

      redis {
        database {
          extractor.duplication.store.id = 1
          preprocessor.duplication.store.id = 2
          key.expiry.seconds = 3600
        }
      }
    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      jobmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1
      scheduler-mode: reactive
      heartbeat.timeout: 8000
      heartbeat.interval: 5000
      state.savepoints.dir: file:///tmp

  transformer-ext:
    enabled: false

    registry: sanketikahub
    repository: transformer-ext
    tag: 1.0.0-GA
    imagePullSecrets: []

    job_classname: in.sanketika.obsrv.transformer.task.TransformerStreamTask

    serviceMonitor: *serviceMonitor

    config: |+
      include file("/data/flink/conf/baseconfig.conf")
      kafka {
        input.topic = "denorm"
        output.transform.topic = "transform"
        output.transform.failed.topic = "transform.failed"
        groupId = "transformer-group"
        producer {
          max-request-size = 5242880
        }
      }

      task {
        consumer.parallelism = 1
        downstream.operators.parallelism = 1
      }
    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      jobmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1
      scheduler-mode: reactive
      heartbeat.timeout: 8000
      heartbeat.interval: 5000
      state.savepoints.dir: file:///tmp

  master-data-processor-ext:
    enabled: false

    registry: sanketikahub
    repository: master-data-processor-ext
    tag: 1.0.0-GA
    imagePullSecrets: []

    job_classname: in.sanketika.obsrv.pipeline.task.MasterDataProcessorStreamTask

    serviceMonitor: *serviceMonitor

    config: |+
      include file("/data/flink/conf/baseconfig.conf")
      kafka {
        input.topic = "masterdata.ingest"
        output.raw.topic = "masterdata.raw"
        output.extractor.duplicate.topic = "masterdata.failed"
        output.failed.topic = "masterdata.failed"
        output.batch.failed.topic = "masterdata.failed"
        event.max.size = "1048576" # Max is only 1MB
        output.invalid.topic = "masterdata.failed"
        output.unique.topic = "masterdata.unique"
        output.duplicate.topic = "masterdata.failed"
        output.denorm.topic = "masterdata.denorm"
        output.transform.topic = "masterdata.transform"
        output.transform.failed.topic = "masterdata.transform.failed"
        stats.topic = "masterdata.stats"
        groupId = "masterdata-pipeline-group"

        producer {
          max-request-size = 5242880
        }
      }

      task {
        window.time.in.seconds = 5
        window.count = 30
        window.shards = 1400
        consumer.parallelism = 1
        downstream.operators.parallelism = 1
      }

      redis {
        database {
          extractor.duplication.store.id = 1
          preprocessor.duplication.store.id = 2
          key.expiry.seconds = 3600
        }
      }

      dataset.type = "master-dataset"
    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      jobmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1
      scheduler-mode: reactive
      heartbeat.timeout: 8000
      heartbeat.interval: 5000
      state.savepoints.dir: file:///tmp

  cache-indexer:
    enabled: false

    registry: sanketikahub
    repository: cache-indexer
    tag: 1.0.0-GA
    imagePullSecrets: []

    job_classname: org.sunbird.obsrv.streaming.CacheIndexerStreamTask

    serviceMonitor: *serviceMonitor

    config: |+
      include file("/data/flink/conf/baseconfig.conf")
      kafka {
        input.topic = "masterdata.ingest"
        output.raw.topic = "masterdata.raw"
        output.extractor.duplicate.topic = "masterdata.failed"
        output.failed.topic = "masterdata.failed"
        output.batch.failed.topic = "masterdata.failed"
        event.max.size = "1048576" # Max is only 1MB
        output.invalid.topic = "masterdata.failed"
        output.unique.topic = "masterdata.unique"
        output.duplicate.topic = "masterdata.failed"
        output.denorm.topic = "masterdata.denorm"
        output.transform.topic = "masterdata.transform"
        output.transform.failed.topic = "masterdata.transform.failed"
        stats.topic = "masterdata.stats"
        groupId = "masterdata-pipeline-group"

        producer {
          max-request-size = 5242880
        }
      }

      task {
        window.time.in.seconds = 5
        window.count = 30
        window.shards = 1400
        consumer.parallelism = 1
        downstream.operators.parallelism = 1
      }

      redis {
        database {
          extractor.duplication.store.id = 1
          preprocessor.duplication.store.id = 2
          key.expiry.seconds = 3600
        }
      }

      dataset.type = "master-dataset"
    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      jobmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1
      scheduler-mode: reactive
      heartbeat.timeout: 8000
      heartbeat.interval: 5000
      state.savepoints.dir: file:///tmp
      state.checkpoint.interval: {{ .Values.checkpoint_interval }}
      state.checkpoint.pause.between.seconds: {{ .Values.checkpoint_pause_between_seconds }}


commonAnnotations:
  reloader.stakater.com/auto: "true"